<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--
Copyright (c) 2006, International Business Machines Corporation and others.
All Rights Reserved.
 -->
<html xmlns="http://www.w3.org/1999/xhtml">

    <head>
        <meta http-equiv="content-type" content="text/html;charset=utf-8" />
        <meta name="generator" content="Adobe GoLive" />
        <title>ICU UTF-8 Design</title>
    </head>

    <body>
        <h1>ICU UTF-8 Design</h1>
        <p align="right">Markus Scherer 2006-apr-13..2006-may-02</p>
        <div align="right">
            <p>Latest version: <a href="http://dev.icu-project.org/cgi-bin/viewcvs.cgi/*checkout*/icuhtml/design/strings/icu_utf8.html">http://dev.icu-project.org/cgi-bin/viewcvs.cgi/*checkout*/icuhtml/design/strings/icu_utf8.html</a></p>
            <p>Previous versions: <a href="http://dev.icu-project.org/cgi-bin/viewcvs.cgi/*checkout*/icuhtml/design/strings/icu_utf8.html">http://dev.icu-project.org/cgi-bin/viewcvs.cgi/icuhtml/design/strings/icu_utf8.html</a></p>
        </div>
        <h2>Objectives</h2>
        <p>Note: &quot;ICU&quot; in this document refers to ICU4C, the C/C++ libraries.</p>
        <p><strong>Goal:</strong> Within ICU's infrastructure, make it easier and more efficient to work with UTF-8 text.</p>
        <p><strong>Non-Goal:</strong> We do not want to switch ICU wholesale from UTF-16 to UTF-8.</p>
        <h2>Background</h2>
        <p>Like much Unicode-aware software, ICU has always worked with 16-bit Unicode text. All of Unicode is supported via UTF-16. (Originally, ICU was designed for UCS-2, but it was updated to proper UTF-16 handling, mostly in 2000-2001.)</p>
        <p>However, much existing C/C++ software was written for byte-based (<code>char *</code>) text processing which is often switched to UTF-8 for globalization, without changing the physical data types. The goal is for such software to take advantage of ICU's services.</p>
        <h2>Summary of Techniques</h2>
        <ul>
            <li><a href="#helpers">Helpers</a> (UTF-8 macros and transformation functions)</li>
            <li><a href="#wrappers">Wrappers</a></li>
            <li><a href="#utf8impl">Dedicated UTF-8 Implementations</a></li>
            <li><a href="#utext"><code>UText</code> Services</a></li>
            <li><a href="#formats">Data Formats</a>
                <ul>
                    <li><a href="#utrie">UTrie</a></li>
                    <li><a href="#rb_strings">Resource Bundle Strings</a></li>
                </ul>
            </li>
        </ul>
        <h2><a id="helpers" name="helpers"></a>Helpers</h2>
        <p>ICU already provides a small number of macros and functions for UTF-8 text processing. So far, these have been provided for convenience and not optimized particularly for performance.</p>
        <h3>Macros</h3>
        <p>Low-level macros for accessing Unicode code points from UTF-8 text are in <a href="http://dev.icu-project.org/cgi-bin/viewcvs.cgi/*checkout*/icu/source/common/unicode/utf8.h">unicode/utf8.h</a></p>
        <p>Currently, the validating (&quot;safe&quot;) code point access macros (like <code>U8_NEXT()</code>) only handle ASCII characters inline and incur a function call for all other characters. With a small amount of additional inline code, they could easily handle most other common characters inline as well. In particular, all 2-byte UTF-8 sequences and 3-byte sequences for U+1000..U+CFFF which all use the full set of trail byte values. Other valid sequences, and error cases, could continue to incur a function call.</p>
        <h3>Transformation Functions</h3>
        <p>ICU provides UTF-8↔UTF-16 transformation functions. (<code>u_strFromUTF8()</code> and <code>u_strToUTF8()</code> in <a href="http://dev.icu-project.org/cgi-bin/viewcvs.cgi/*checkout*/icu/source/common/unicode/ustring.h">unicode/ustring.h</a>) The same inlining as for the macros could speed these up as well.</p>
        <h2><a id="wrappers" name="wrappers"></a>Wrappers</h2>
        <p>Wrapper functions could be added in ICU, or on top of ICU, providing existing functionality with UTF-8 text APIs. Input UTF-8 strings would be transformed to UTF-16, and output strings transformed back to UTF-8. (This is how Windows NT/2000/XP... works with non-Unicode &quot;ANSI&quot; strings.) Wrappers are simple and fast to write and test.</p>
        <p>In some cases, it may make sense to provide a single wrapper for a sequence of function calls. For example, a single function could be provided for fetching a message format string from a resource bundle and using it to instantiate a formatter, without transforming the string at all.</p>
        <p>Issues:</p>
        <ul>
            <li>Performance: Wrappers are generally ok until performance proves to be a problem.</li>
            <li>Memory: Temporary UTF-16 versions of strings could be held in stack buffers, with allocation when necessary.</li>
            <li>Ownership: Output strings would have to be written to caller-owned buffers with the usual ICU semantics, or returned as objects whose ownership passes to the caller, or placed into another form of by-reference parameter.</li>
            <li>Indexes: If indexes into UTF-8 text need to be communicated, then <code>UText</code> APIs are more appropriate.</li>
        </ul>
        <h2><a id="utf8impl" name="utf8impl"></a>Dedicated UTF-8 Implementations</h2>
        <p>The highest performance, and the largest amount of code duplication and development, can be expected from dedicated UTF-8 implementations parallel to existing UTF-16 implementations. They also require full testing. Hopefully, existing test code can be restructured to test both implementations with relatively little extra code.</p>
        <h3>Converters</h3>
        <p>ICU already has a function <code>ucnv_convertEx()</code> (see <a href="http://dev.icu-project.org/cgi-bin/viewcvs.cgi/*checkout*/icu/source/common/unicode/ucnv.h">unicode/ucnv.h</a>) which takes two UConverter objects and converts between to charsets, one of which can be UTF-8. It pivots through UTF-16, via a buffer that can be supplied by the caller (for full functionality) or internally on the stack (for convenience). The <code>ucnv_convert()</code> function does the same, but takes charset names instead of converter objects, which makes it easier to use but limits its applicability.</p>
        <p>Character conversion tends to be performance-sensitive. These conversion functions provide a few opportunities for improvement, even without new API functions:</p>
        <ul>
            <li>A <em>relatively </em>simple optimization could use a dedicated UTF-8↔charset conversion implementation for &quot;normal&quot; charsets which are the most common and tend to be the most important for performance. (Using the existing .cnv data file format.) The functions would continue to pivot through UTF-16 for more complicated charsets (e.g., ISO 2022 variants or SCSU etc.).</li>
            <li>A more ambitious optimization could use a different data structure for the conversion tables, specifically designed for conversion to/from UTF-8.
                <ul>
                    <li>Conversion to UTF-8 could yield UTF-8 bytes directly rather than UChars which have to be transformed to UTF-8 bytes.</li>
                    <li>Conversion from UTF-8 could use a trie with a 9:6:6 input bit distribution rather than the current 11:6:4, to avoid assembling complete code point values and immediately taking them apart for the trie lookup.<br />
                        Issue: Currently, for non-SBCS output, the third-stage values include a bit for each of the associated 16 values for whether it is a roundtrip mapping or not. (If it's not, it's a fallback mapping, or no mapping at all.) With 64 third-stage values, this becomes impractical. A possible solution is to handle all fallback mappings via the extension table.</li>
                    <li>In this case, for usability, there should be (unoptimized) implementations for UTF-16↔charset conversion using the UTF-8-optimized data structures, and for UTF-8↔charset conversion using the traditional data structures. Otherwise, including the wrong kind of .cnv file would cause surprising runtime failures.</li>
                </ul>
            </li>
        </ul>
        <h3>Fast Opening of a UTF-8 Converter</h3>
        <p>Opening any converter in ICU involves a lookup in the alias table. When using <code>ucnv_convertEx()</code> with a UTF-8 converter, such a converter may need to be opened or cached+cloned very often. If this is a performance problem, then several remedies are possible:</p>
        <p>a) There may be ways to improve the performance of the <code>ucnv_open()</code> implementation.</p>
        <p>b) A new converter option in the converter name string (e.g., &quot;UTF-8,noalias&quot; or &quot;UTF-8,direct&quot;) could cause (only) the alias table lookup to be bypassed. (Thanks to George Rhoten for this idea.)</p>
        <p>c) A new variant of  <code>ucnv_open()</code> could be added, for example <code>ucnv_openAlgorithmic(UConverterType algorithmicType, UErrorCode *pErrorCode)</code>. It would avoid all string parsing and table searches.</p>
        <p>There are already variants of the <code>ucnv_convert()</code> function which provide for UTF-8↔charset conversions by specifying an enum constant for UTF-8 instead of the name &quot;UTF-8&quot; (which would have to be looked up): <code>ucnv_fromAlgorithmic()</code> and <code>ucnv_toAlgorithmic()</code>. Unfortunately, they are a little too convenient: Since they specify the algorithmic charset with a simple enum constant, they do not allow customizing the converter, for example by setting a certain callback function for error handling, and they do not support fetching the last bad input sequence. We should consider adding a new API function for opening an algorithmic converter, which is faster than opening a converter from a name because it bypasses the alias table and a search in an internal table of names of algorithmic converters. Such a converter could then be used as usual in all conversion APIs.</p>
        <h2><a id="utext" name="utext"></a>UText Services</h2>
        <p>ICU 3.4 added, and ICU 3.6 will refine, the <code>UText</code> API for abstract text access. It allows ICU services that have been modified to use it to work with a variety of text encodings, including UTF-8. In addition, it also supports discontiguous text storage, writable strings, and storage-native indexes. For a complete overview, see the <a href="http://icu.sourceforge.net/userguide/utext.html">ICU User Guide UText</a> chapter.</p>
        <p>There is work in progress, or otherwise possible:</p>
        <ul>
            <li>The API is being modified for ICU 3.6 to support a larger variety of text types more efficiently. After ICU 3.6, the <code>UText</code> API should be stable.</li>
            <li>The UTF-8 <code>UText</code> provider should be profiled and tuned for performance. (Some of the macro inlining ideas may be applicable.)</li>
            <li>The UTF-8 <code>UText</code> provider should be made writable.</li>
            <li>There will likely be multiple UTF-8 providers that only differ in how the UTF-8 text is stored, but perform the same transformations to/from UTF-16. For example, UTF-8 could be stored in <code>char *</code> buffers, in STL <code>string</code> objects, or in files. For this, there should be an additional intra-<code>UText</code>-provider interface which serves chunks of bytes, so that there needs to be only one UTF-8 transformation implementation.</li>
        </ul>
        <h2><a id="formats" name="formats"></a>Data Formats</h2>
        <h3><a id="utrie" name="utrie"></a>UTrie</h3>
        <p>ICU uses <code>UTrie</code> data structures for looking up values associated with Unicode code points. A <code>UTrie</code> consists of an index table and a data table. An input BMP code point (16 bits) is segmented into the upper 11 bits, for access into the index table, and the lower 5 bits, for access into the data table, relative to where the index value points. (Supplementary code points are looked up in two steps from their surrogate pair code units.)</p>
        <p>Given the <code>UTrie</code> design, the split of the input code units could have been anywhere from 15:1 to 7:9. At the time (2001), there existed two ICU data files using <code>UTrie</code>s (uprops.icu and unorm.icu), and the 11:5 split yielded the smallest sizes of these two files.</p>
        <p>For highly optimized UTF-8 text processing using <code>UTrie</code>s, it would be better to use a 10:6 split: For multi-byte characters, the code point would not have to be assembled completely; instead, the last byte could be used directly for the data table lookup, while the other bytes would be only partially assembled for the index table lookup.</p>
        <p>UTrie-using code that is not very performance-sensitive, especially the <code>utrie_swap()</code> function and the icupkg tool, could be made to work with any valid trie shift and trie index shift values at runtime.</p>
        <blockquote>
            <p>For example, add a struct <code>UTrieConstants</code> and function <code>utrie_setConstants(trieConstants, trieShift, trieIndexShift)</code> and use these constants instead of the hardcoded UTRIE_ values.</p>
        </blockquote>
        <p>Changing the <code>UTrie</code> bit distribution results in an incompatible change in ICU data file formats. Such a modified ICU build is possible, but would have to be carefully kept separate from a standard build, for example by using a library suffix, to avoid mix-ups.</p>
        <h3><a id="rb_strings" name="rb_strings"></a>Resource Bundle Strings</h3>
        <p>ICU resource bundles store UTF-16 strings for most efficient use in UTF-16 text processing. The most basic API function returns a <code>const UChar *</code> pointer to the memory-mapped string. This is of course inconvenient if such a string is to be used immediately in a UTF-8 context.</p>
        <p>At a minimum, a convenience API could combine the string lookup with a transformation to UTF-8, writing to a caller-provided <code>char *</code> buffer or some kind of string object. (<code>ures_getUTF8String()</code>)</p>
        <p>The UTF-8 string <em>length</em> could be stored alongside the UTF-16 strings, so that a transformation would know exactly how much to allocate. The length could be omitted for strings with at most 3 UChars, for example, to reduce the relative overhead. (If a string object type didn't support aliasing existing memory but always required a copy, this might be sufficient.)</p>
        <p>Optionally (that is, with a command-line switch), strings could be stored in both UTF-8 and UTF-16 forms, and the UTF-8 string accessor could return the pointer rather than copying the string to an output buffer. (Convenient but costly.) The options could be associated with an increasing level of &quot;UTF-8-ness&quot;:</p>
        <ul>
            <li>0: Store all strings in UTF-16 only. (Current format.)</li>
            <li>1: Store UTF-16 strings with an additional field with the UTF-8 length.</li>
            <li>2: Store strings in both UTF-16 and UTF-8.</li>
            <li>3: Store strings only in UTF-8. For backward compatibility, set each string's UTF-16 length field to 0, store a UTF-16 NUL UChar for termination, and store an additional field for the UTF-16 length. Add a <code>ures_getUTF16String()</code> function that can write to a destination buffer for conversion. (<code>ures_getUnicodeString()</code> would continue to work but not <code>ures_getString()</code>.)</li>
        </ul>
        <p>Alternatively, UTF-8 strings could be stored as binary objects and the <code>genrb</code> tool could be changed to always NUL-terminate binaries (and intvectors, for possible use for UTF-32 strings). The resource bundle source files would have to be prepared by another tool, or <code>genrb</code> could be further modified to accept a new resource type (for example, :utf8) and generate binaries. This is extensible to UTF-32 (:utf32) strings stored as intvectors. The <code>ures_getUTFxyString()</code> functions could convert from the storage UTF to the requested one when necessary.</p>
        <p>There could even be a new resource type for UTF-8 strings. A drawback (shared with binary storage and with level 3 &quot;UTF-8-ness&quot; above) would be that such strings could not be returned from the standard, basic API function (as a <code>const UChar *</code>) because of ownership issues. (It is possible to cache transformed strings, but at the cost of mutexing the semantically-const bundle.)</p>
        <p>There should be a new API function (<code>ures_getUTF16String()</code>) which converts a string to a UTF-16 destination buffer if its internal storage is not UTF-16.</p>
        <p>It would be very confusing to not be able to access all strings in either form.</p>
        <p></p>
    </body>

</html>