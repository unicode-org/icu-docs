<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Unicode Conversion Extensions</title>
</head>

<body bgcolor="#FFFFFF">

<h1>Unicode Conversion Extensions</h1>

<table border="0" cellspacing="0" cellpadding="4">
  <tr>
    <td>Subject:</td>
    <td>Unicode Conversion Extensions</td>
  </tr>
  <tr>
    <td>Author:</td>
    <td>Markus Scherer, IBM</td>
  </tr>
  <tr>
    <td>Date:</td>
    <td>2003-06-03</td>
  </tr>
  <tr>
    <td>Latest Version:</td>
    <td><a href="http://oss.software.ibm.com/cvs/icu/~checkout~/icuhtml/design/conversion/conversion_extensions.html">http://oss.software.ibm.com/cvs/icu/~checkout~/icuhtml/design/conversion/conversion_extensions.html</a></td>
  </tr>
  <tr>
    <td>CVS:</td>
    <td><a href="http://oss.software.ibm.com/cvs/icu/icuhtml/design/conversion/conversion_extensions.html">http://oss.software.ibm.com/cvs/icu/icuhtml/design/conversion/conversion_extensions.html</a>
    </td>
  </tr>
</table>
<h2>Issues</h2>
<ul>
  <li>There are often several Unicode conversion tables per charset.</li>
  <li>Ideally, ICU would support as many of them out of the box as possible.</li>
  <li>Out of the box, ICU already supports &gt;200 conversion tables, taking up 
    5MB of its 8MB data. There are hundreds more conversion tables in the <a href="http://oss.software.ibm.com/icu/charset/">ICU 
    charset repository</a>.</li>
  <li>ICU's data structures (.ucm source files and .cnv binaries) do not  
    currently support conversion of multiple characters/code points at once. For  
    some legacy and some newer charsets, such many-to-one mappings are required. 
    <ul>
      <li>JIS X 0213 maps some JIS characters to multiple Unicode code points  
        (including combining enclosing marks). Future CJK conversion tables are 
        expected to map some characters to Unicode code point sequences 
        including variation selectors.</li>
      <li>MacOS maps some Unicode code points to multiple charset characters 
        according to their canonical or compatibility decompositions (e.g., 
        Unicode ff ligature to 2 'f's).</li>
    </ul>
  </li>
</ul>
<h2>Ideas</h2>
<h3>Converter + Transliterator</h3>
<p>A long-standing idea is to combine ICU conversion with ICU 
transforms/transliteration. The Transliterator service is very flexible and 
handles m:n mappings between sequences of Unicode code points. The ICU converter 
would be customized to map to/from special (PUA) code points matching the 
transliterator/transform rules.</p>
<p>Benefits:</p>
<ul>
  <li>Combination of existing APIs.</li>
  <li>Handling of complex transformations like Arabic shaping or Indic syllable 
    transformation.</li>
  <li>Possibly even BiDi reordering.</li>
</ul>
<p>Issues:</p>
<ul>
  <li>The combination of a converter and a transliterator would probably require 
    a different API than the regular conversion API, which limits the use of the 
    new API to new application code taking advantage of it.</li>
  <li>Offsets would probably not be able to be provided.</li>
  <li>Complex cases would probably require chunking of input text into lines or 
    paragraphs.</li>
</ul>
<p>Jitterbug: <a href="http://www.jtcsv.com/cgibin/icu-bugs?findid=2234">2234</a> 
(originally 618, but that was mistakenly used to commit changes for a different 
bug)</p>
<h3>Converter Extension</h3>
<p>The obvious alternative is to extend the conversion data structures to 
support more of what ICU users need. Such an extension would be designed to work 
within the existing conversion API, which would be immediately enabled for all 
applications that upgrade to a newer ICU release. While more complex 
transformations would not be designed into such extensions, m:n mappings should 
be possible and offsets should be able to be provided.</p>
<p>If we could combine multiple .cnv files into one, then we could save space 
while increasing the set of supported charsets.</p>
<p>Jitterbug: <a href="http://www.jtcsv.com/cgibin/icu-bugs?findid=2404">2404</a></p>
<p>The section below contains ideas for binary data structures.</p>
<h2>Extension Data Structure</h2>
<p>The basic idea is to handle most mappings via the existing state table (to 
Unicode) and trie (from Unicode). When an unmappable result or a special, new 
marker is found, then an additional table is searched. A new marker in a 
fromUnicode table could be a result byte sequence with a trailing 00 byte, for 
example. Using a special marker would avoid slowing down processing of 
unmappable characters, which might be common when output-escaping XML.</p>
<p>An additional table need not be designed for high performance if its mappings 
are rarely used. The most compact would be to store a list of pairs of (Unicode 
code units, charset bytes) where the unit/byte sequences are of variable length. 
Variable length entries would allow multiple-character mappings.</p>
<p>The list would be segmented into the three main types of mappings: First 
fallbacks (only fromUnicode), then roundtrips, then reverse fallbacks (only 
toUnicode). Only relevant segments would be searched.</p>
<p>One simple optimization would be to store a fixed-width variant of the list  
for those cases where there is only one 16-bit Unicode code unit (BMP character)  
and only one or two charset bytes. This would cover many simple cases, save 
space, and allow much faster searches.</p>
<p>The lists could simply be searched linearly. The use of the same lists for 
both mapping directions and of per-mapping-type segments saves space but makes 
it hard to provide for faster access. The expectation is that this is acceptable 
because this is for legacy charsets and for infrequently used characters. Full 
performance (except for m:n mappings) is available by using a separate .cnv file 
per table variant.</p>
<pre>
// array of indexes to extension table segments
// each index is a 16-bit-addressed index from index[0]
// indexes[0], [1], [2] contain indexes to the fixedTable[] segments for
// fallback/roundtrip/reverse fallback mappings
// [0] also indicates the length of the indexes[] array before the conversion data (==16)
// [3], [4], [5] contain indexes to the variableTable[] segments
// [3] also indicates the first index after the fixedTable[]
// [6] indicates the first index after the variableTable[]
uint16_t indexes[16];

// each two words contain a Unicode BMP code point and 1..2 charset bytes,
// both encoded as platform-endian 16-bit words
uint16_t fixedTable[(i3-i0)*2];

// each pair of (Unicode units, charset bytes) is encoded as follows:
// uint16_t lengths;
// UChar unicodeUnits[lengths bits 3..0]
// uint8_t charsetBytes[lengths bits 7..4]
// padded to 16-bit boundary; total number of 16-bit units including lengths word: lengths bits 15..8
uint16_t variableTable[i6];

</pre>
<p>A single .cnv file may contain multiple such data structures, one per  
conversion table variant. A Shift-JIS .cnv table would use the regular state  
table + trie for all common mappings and one extension data structure per  
variant. Only one variant would be active for a particular converter. A basic 
requirement is that the state tables of all variants must be identical (at least 
for the set of legal byte sequences).</p>
<p>The  
canonical names of (or some other IDs for) the variants would probably need to  
be stored with the extensions for selection, and the alias table would have to  
select the common .cnv file with an option for the variant name/ID. It would be  
most convenient if makeconv could take multiple .ucm files and automatically  
generate a combined .cnv file. Real canonical names stored in the .cnv file 
would be easiest to automate and best for ucnv_getName() returning a const char 
*.</p>

</body>

</html>
