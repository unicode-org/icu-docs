<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Unicode Conversion Extensions</title>
</head>

<body bgcolor="#FFFFFF">

<h1>Unicode Conversion Extensions</h1>

<table border="0" cellspacing="0" cellpadding="4">
  <tr>
    <td>Subject:</td>
    <td>Unicode Conversion Extensions</td>
  </tr>
  <tr>
    <td>Author:</td>
    <td>Markus Scherer, IBM</td>
  </tr>
  <tr>
    <td>Date:</td>
    <td>2003-06-03..04</td>
  </tr>
  <tr>
    <td>Latest Version:</td>
    <td><a href="http://oss.software.ibm.com/cvs/icu/~checkout~/icuhtml/design/conversion/conversion_extensions.html">http://oss.software.ibm.com/cvs/icu/~checkout~/icuhtml/design/conversion/conversion_extensions.html</a></td>
  </tr>
  <tr>
    <td>CVS:</td>
    <td><a href="http://oss.software.ibm.com/cvs/icu/icuhtml/design/conversion/conversion_extensions.html">http://oss.software.ibm.com/cvs/icu/icuhtml/design/conversion/conversion_extensions.html</a>
    </td>
  </tr>
</table>
<h2>Issues</h2>
<ul>
  <li>There are often several Unicode conversion tables per charset.</li>
  <li>Ideally, ICU would support as many of them out of the box as possible.</li>
  <li>Out of the box, ICU already supports &gt;200 conversion tables, taking up 
    5MB of its 8MB data. There are hundreds more conversion tables in the <a href="http://oss.software.ibm.com/icu/charset/">ICU 
    charset repository</a>.</li>
  <li>ICU's data structures (.ucm source files and .cnv binaries) do not  
    currently support conversion of multiple characters/code points at once. For  
    some legacy and some newer charsets, such many-to-one mappings are required. 
    <ul>
      <li>JIS X 0213 maps some JIS characters to multiple Unicode code points  
        (including combining enclosing marks). Future CJK conversion tables are 
        expected to map some characters to Unicode code point sequences 
        including variation selectors.</li>
      <li>MacOS maps some Unicode code points to multiple charset characters 
        according to their canonical or compatibility decompositions (e.g., 
        Unicode ff ligature to 2 'f's).</li>
    </ul>
  </li>
</ul>
<h2>Ideas</h2>
<h3>Converter + Transliterator</h3>
<p>A long-standing idea is to combine ICU conversion with ICU 
transforms/transliteration. The Transliterator service is very flexible and 
handles m:n mappings between sequences of Unicode code points. The ICU converter 
would be customized to map to/from special (PUA) code points matching the 
transliterator/transform rules.</p>
<p>Benefits:</p>
<ul>
  <li>Combination of existing APIs.</li>
  <li>Handling of complex transformations like Arabic shaping or Indic syllable 
    transformation.</li>
  <li>Possibly even BiDi reordering.</li>
</ul>
<p>Issues:</p>
<ul>
  <li>The combination of a converter and a transliterator would probably require 
    a different API than the regular conversion API, which limits the use of the 
    new API to new application code taking advantage of it.</li>
  <li>Offsets would probably not be able to be provided.</li>
  <li>Complex cases would probably require chunking of input text into lines or 
    paragraphs.</li>
</ul>
<p>Jitterbug: <a href="http://www.jtcsv.com/cgibin/icu-bugs?findid=2234">2234</a> 
(originally 618, but that was mistakenly used to commit changes for a different 
bug)</p>
<h3>Converter Extension</h3>
<p>The obvious alternative is to extend the conversion data structures to 
support more of what ICU users need. Such an extension would be designed to work 
within the existing conversion API, which would be immediately enabled for all 
applications that upgrade to a newer ICU release. While more complex 
transformations would not be designed into such extensions, m:n mappings should 
be possible and offsets should be able to be provided.</p>
<p>If we could combine multiple .cnv files into one, then we could save space 
while increasing the set of supported charsets.</p>
<p>Jitterbug: <a href="http://www.jtcsv.com/cgibin/icu-bugs?findid=2404">2404</a></p>
<p>The section below contains ideas for binary data structures.</p>
<h2>Extension Data Structure</h2>
<p>The basic idea is to handle most mappings via the existing state table (to 
Unicode) and trie (from Unicode). When an unmappable result or a special, new 
marker is found, then an additional table is searched. A new marker in a 
fromUnicode table could be a result byte sequence with a trailing 00 byte, for 
example. Using a special marker would avoid slowing down processing of 
unmappable characters, which might be common when output-escaping XML.</p>
<p>An additional table need not be designed for high performance if its mappings 
are rarely used. The most compact would be to store a list of pairs of (Unicode 
code units, charset bytes) where the unit/byte sequences are of variable length. 
Variable length entries would allow multiple-character mappings.</p>
<p>The list would be segmented into the three main types of mappings: First 
fallbacks (only fromUnicode), then roundtrips, then reverse fallbacks (only 
toUnicode). Only relevant segments would be searched.</p>
<p>One simple optimization would be to store a fixed-width variant of the list  
for those cases where there is only one 16-bit Unicode code unit (BMP character)  
and only one or two charset bytes. This would cover many simple cases, save 
space, and allow much faster searches.</p>
<p>The lists could simply be searched linearly. The use of the same lists for 
both mapping directions and of per-mapping-type segments saves space but makes 
it hard to provide for faster access. The expectation is that this is acceptable 
because this is for legacy charsets and for infrequently used characters. Full 
performance (except for m:n mappings) is available by using a separate .cnv file 
per table variant.</p>
<pre>
// array of indexes to extension table segments
// each indexes[i] is a 16-bit-addressed index from the start of indexes itself
// indexes[0], [1], [2] contain indexes to the fixedTable[] segments for
// fallback/roundtrip/reverse fallback mappings
// [0] also indicates the length of the indexes[] array before the
//     conversion data (==2*16 uint16_t==16 uint32_t)
// [3], [4], [5] contain indexes to the variableTable[] segments
// [3] also indicates the first index after the fixedTable[]
// [6] indicates the first index after the variableTable[]
// [7]..[14] reserved
// [15] number of bytes for the entire extension structure
uint32_t indexes[16];

// each two words contain a Unicode BMP code point and 1..2 charset bytes,
// both encoded as platform-endian 16-bit words
uint16_t fixedTable[(i3-i0)*2];

// each pair of (Unicode units, charset bytes) is encoded as follows:
// uint16_t lengths;
// UChar unicodeUnits[lengths bits 3..0]
// uint8_t charsetBytes[lengths bits 7..4]
// padded to 16-bit boundary;
// total number of 16-bit units including lengths word: lengths bits 15..8
uint16_t variableTable[i6];
</pre>
<h3>Multiple Extension Tables per File</h3>
<p>A single .cnv file may contain multiple such data structures, one per   
conversion table variant. A Shift-JIS .cnv table would use the regular state   
table + trie for all common mappings and one extension data structure per   
variant. Only one variant would be active for a particular converter. A basic  
requirement is that the state tables of all variants must be identical (at least  
for the set of legal byte sequences).</p> 
<p>The  
canonical names of (or some other IDs for) the variants would probably need to  
be stored with the extensions for selection, and the alias table would have to  
select the common .cnv file with an option for the variant name/ID. It would be  
most convenient if makeconv could take multiple .ucm files and automatically  
generate a combined .cnv file. Real canonical names stored in the .cnv file 
would be easiest to automate and best for ucnv_getName() returning a const char 
*.</p>

<h3>Single Extension Tables per File</h3>
<p>Alternatively, one .cnv file may contain at most one extension data 
structure. Multiple extension files would refer by name to the base table. 
Variants could be added much more easily than if all are stored in a single file 
because an additional variant just requires one more .cnv file. Multiple 
Shif-JIS variants would refer to the same sjisbase.cnv which itself would not be 
listed as an available converter.</p>
<p>It may be useful to allow one extension data structure in a regular .cnv file 
so that conversion tables with complex mappings need not be split into 
base+extension files.</p>
<p>Using separate files for base and extension requires the base to not contain 
mappings for anything that any of the extensions covers. This is not too hard 
with a conservative set of mappings for the base (although a conservative base 
set increases the size of extensions), but it makes it very difficult to use a 
special extension marker in the base table. In this case it may be necessary to 
always check the extension table when an input sequence is legal but unmappable.</p>
<p>The alias table would be very simple: One regular canonical name per variant, 
without any options syntax. The extension .cnv file would contain the associated 
base table's canonical name.</p>
<p>The makeconv tool would also be simpler than with multiple variants per .cnv  
file. Initially it could just read the .ucm source for the extension and  
generate a .cnv file with a new internal format. Eventually it should read the  
base .ucm or .cnv file as well and verify that the base does not contain  
conflicting mappings, as well as verifying that the charset byte sequences are  
legal. It would never have to read all variants at once.</p> 
<p>To simplify .ucm file maintenance, makeconv should accept complete conversion 
tables for extensions, including the base mappings, and simply ignore the base 
mappings. This of course requires that makeconv read the base table along with 
the extension .ucm file. Otherwise one would have to slice the extension part 
off the base mappings manually or with a separate tool.</p>
<h3>Minimum Conversion Table Size</h3>
<p>An interesting by-product of these ideas is that the extension data structure 
is designed to be compact, not fast. For conversions where speed is not 
important but where basic support is desired, there could be one regular .cnv 
file without any mappings, only with an all-unassigned state table (and empty 
trie) to document the charset structure, plus an extension table in the same or 
a different file with the actual mappings.</p>
<p>Another way to save space is to combine multiple conversion tables that are 
not logically related in that they are not variants of what is supposed to be 
the same charset. For example, several SBCS tables could share a base table for 
invariant ASCII or EBCDIC mappings and use extensions for all the other 
characters. Even though SBCS tables are small, extension tables for some 150 
mappings would be even smaller.</p>

</body>

</html>
